/*--------------------------------------------------------------------------
   Author: Thomas Nowotny
  
   Institute: Center for Computational Neuroscience and Robotics
              University of Sussex
	      Falmer, Brighton BN1 9QJ, UK 
  
   email to:  T.Nowotny@sussex.ac.uk
  
   initial version: 2010-02-07
  
--------------------------------------------------------------------------*/


//----------------------------------------------------------------------------
/*!  \page UserManual User Manual

\latexonly
\vspace{0cm}\mbox{}\vspace{0cm}
\endlatexonly

\section Contents
- \ref sIntro\n
- \subpage sectDefiningNetwork
- \subpage sectNeuronModels
- \subpage sectSynapseModels
- \subpage sect_postsyn
- \subpage sectCurrentSourceModels
- \subpage sectCustomUpdate
- \subpage subsect34
- \subpage sectVariableInitialisation
- \subpage sectVariableReferences
- \subpage sectSparseConnectivityInitialisation

\section sIntro Introduction

GeNN is a software library for facilitating the simulation of neuronal
network models on NVIDIA CUDA enabled GPU hardware. It was designed
with computational neuroscience models in mind rather than artificial
neural networks. The main philosophy of GeNN is two-fold:

1. GeNN relies heavily on code generation to make it very flexible and
   to allow adjusting simulation code to the model of interest and the GPU
   hardware that is detected at compile time.
2. GeNN is lightweight in that it provides code for running models of
   neuronal networks on GPU hardware but it leaves it to the user to
   write a final simulation engine. It so allows maximal flexibility to
   the user who can use any of the provided code but can fully choose,
   inspect, extend or otherwise modify the generated code. They can also
   introduce their own optimisations and in particular control the data
   flow from and to the GPU in any desired granularity.

This manual gives an overview of how to use GeNN for a novice user and
tries to lead the user to more expert use later on. With that we jump
right in.
\n


-----
\link ReleaseNotes Previous\endlink | \link UserManual Top\endlink | \link sectDefiningNetwork Next\endlink
*/


//----------------------------------------------------------------------------
/*!  \page sectDefiningNetwork Defining a network model

\add_toggle_cpp
A network model is defined by the user by providing the function 
\code{.cc}
void modelDefinition(ModelSpec &model) 
\endcode
in a separate file, such as `MyModel.cc`. In this function, the following
tasks must be completed:
\end_toggle
\add_toggle_python
A network model is defined as follows:
\end_toggle
1. \add_cpp_python_text{The name of the model must be defined,A pygenn.GeNNModel must be created with a name and a default precision (see \ref floatPrecision)}:
   \add_toggle_code_cpp
   model.setName("MyModel");
   \end_toggle_code
   \add_toggle_python
   \code
   model = GeNNModel("float", "YourModelName")
   \endcode
   By default the model will use a hardware-accelerated code-generation backend if it is available. However, this can be overriden using the `backend` keyword argument. For example, the single-threaded CPU backend could be manually selected with:
   \code
   model = GeNNModel("float", "YourModelName", 
                     backend="SingleThreadedCPU")
   \endcode
   \end_toggle
2. Neuron populations (at least one) must be added (see \ref subsect11).
   The user may add as many neuron populations as they wish. However,
   before this breaking point is reached, GeNN will make all necessary
   efforts in terms of block size optimisation to accommodate the defined
   models. All populations must have a unique name.

3. Synapse populations (zero or more) can be added (see \ref subsect12).

4. Current sources (zero or more) can be added to neuron populations (see \ref defining_current_source).
5. Custom updates (zero or more) can be added (see \ref defining_custom_updates). 

\note
If your model requires more memory than your GPU has available, there will be a warning but GeNN will not fail.

\section subsect11 Defining neuron populations
Neuron populations are added using the function
\add_toggle_code_cpp
model.addNeuronPopulation<NeuronModel>(name, num, paramValues, varInitialisers);
\end_toggle_code
\add_toggle_code_python
model.add_neuron_population(pop_name, num_neurons, neuron, param_space, var_space)
\end_toggle_code
where the arguments are:
\arg \add_cpp_python_text{`NeuronModel`: Template argument specifying the type of neuron model. These should be derived off NeuronModels::Base and can either be one of the standard models or user-defined,`neuron`: The type of neuron model. This should either be a string containing the name of a built in model or user-defined neuron type returned by pygenn.genn_model.create_custom_neuron_class} (see \ref sectNeuronModels).
\arg \add_cpp_python_text{`const string &name`,`pop_name`}: Unique name of the neuron population
\arg \add_cpp_python_text{`unsigned int size`,`num_neurons`}: number of neurons in the population
\arg \add_cpp_python_text{`NeuronModel::ParamValues paramValues`: Parameters, `param_space`: Dictionary containing parameters} of this neuron type
\arg \add_cpp_python_text{`NeuronModel::VarValues varInitialisers`: Initial, `var_space`: Dictionary containing initial} values or initialisation snippets for variables of this neuron type (see \ref sectVariableInitialisation)

The user may add as many neuron populations as the model necessitates.
They must all have unique names. The possible values for the arguments,
predefined models and their parameters and initial values are detailed
\ref sectNeuronModels below.

\section subsect12 Defining synapse populations

Synapse populations are added with the function
\add_toggle_code_cpp
model.addSynapsePopulation<WeightUpdateModel, PostsynapticModel>(name, mType, delay, preName, postName, 
                                                                 weightParamValues, weightVarValues, weightPreVarInitialisers, weightPostVarInitialisers,
                                                                 postsynapticParamValues, postsynapticVarValues, connectivityInitialiser);
\end_toggle_code
\add_toggle_code_python
model.add_synapse_population(pop_name, matrix_type, delay_steps,
                             source, target, w_update_model, wu_param_space,
                             wu_var_space, wu_pre_var_space,
                             wu_post_var_space, postsyn_model,
                             ps_param_space, ps_var_space,
                             connectivity_initialiser
\end_toggle_code
where the arguments are
\arg \add_cpp_python_text{`WeightUpdateModel`: Template parameter specifying the type of weight update model. These should be derived off WeightUpdateModels::Base and can either be one of the standard models or user-defined,`w_update_model`: The type of weight update model. This should either be a string containing the name of a built in model or user-defined weight update model returned by pygenn.genn_model.create_custom_weight_update_class} (see \ref sectSynapseModels).
\arg \add_cpp_python_text{`PostsynapticModel`: Template parameter specifying the type of postsynaptic integration model. These should be derived off PostsynapticModels::Base and can either be one of the standard models or user-defined,`postsyn_model`: The type of postsynaptic model. This should either be a string containing the name of a built in model or user-defined postsynaptic model returned by pygenn.genn_model.create_custom_postsynaptic_class} (see \ref sect_postsyn).
\arg \add_cpp_python_text{`const string &name`,`pop_name`}: The name of the synapse population
\arg \add_cpp_python_text{`::SynapseMatrixType mType`: How,`matrix_type`: String specifying how} the synaptic matrix is stored. See \ref subsect34 for available options.
\arg \add_cpp_python_text{`unsigned int delay`,`delay_steps`}: Homogeneous (axonal) delay for synapse population (in terms of the simulation time step `DT`). 
\arg \add_cpp_python_text{`const string preName`: Name, `source`: pygenn.NeuronGroup or name} of the (existing!) presynaptic neuron population.
\arg \add_cpp_python_text{`const string postName`: Name, `target`: pygenn.NeuronGroup or name} of the (existing!) postsynaptic neuron population.
\arg \add_cpp_python_text{`WeightUpdateModel::ParamValues weightParamValues`: The, `wu_param_space`: Dictionary containing the} parameter values (common to all synapses of the population) for the weight update model. 
\arg \add_cpp_python_text{``WeightUpdateModel::VarValues weightVarInitialisers`: The, `wu_var_space`: Dictionary containing the} initial values or initialisation snippets for the weight update model's state variables (see \ref sectVariableInitialisation)
\arg \add_cpp_python_text{`WeightUpdateModel::PreVarValues weightPreVarInitialisers`: The, `wu_pre_var_space`: Dictionary containing the} initial values or initialisation snippets for the weight update model's presynaptic state variables (see \ref sectVariableInitialisation)
\arg \add_cpp_python_text{`WeightUpdateModel::PostVarValues weightPostVarInitialisers`: The, `wu_post_var_space`: Dictionary containg the} initial values or initialisation snippets for the weight update model's postsynaptic state variables (see \ref sectVariableInitialisation)
\arg \add_cpp_python_text{`PostsynapticModel::ParamValues postsynapticParamValues`: The,`ps_param_space`: Dictionary containing the} parameter values (common to all postsynaptic neurons) for the postsynaptic model. 
\arg \add_cpp_python_text{`PostsynapticModel::VarValues postsynapticVarInitialisers`: The, `ps_var_space`: Dictionary containing the} initial values or initialisation snippets for variables for the postsynaptic model's state variables (see \ref sectVariableInitialisation)
\arg \add_cpp_python_text{`InitSparseConnectivitySnippet::Init connectivityInitialiser`,`connectivity_initialiser`}: Optional argument, specifying the initialisation snippet for synapse population's sparse connectivity (see \ref sectSparseConnectivityInitialisation).

\add_toggle_cpp
The ModelSpec::addSynapsePopulation() function returns a pointer to the newly created SynapseGroup object which can be further configured, namely with:
- SynapseGroup::setMaxConnections() and SynapseGroup::setMaxSourceConnections() to configure the maximum number of rows and columns respectively allowed in the synaptic matrix - this can improve performance and reduce memory usage when using SynapseMatrixConnectivity::SPARSE connectivity (see \ref subsect34).
\note
When using a sparse connectivity initialisation snippet, these values are set automatically.
- SynapseGroup::setMaxDendriticDelayTimesteps() sets the maximum dendritic delay (in terms of the simulation
     time step `DT`) allowed for synapses in this population. No values larger than this should be passed to the delay parameter of the `addToDenDelay` function in user code (see \ref sect34).
- SynapseGroup::setSpanType() sets how incoming spike processing is parallelised for this synapse group. The default SynapseGroup::SpanType::POSTSYNAPTIC is nearly always the best option, but SynapseGroup::SpanType::PRESYNAPTIC may perform better when there are large numbers of spikes every timestep or very few postsynaptic neurons.}
- SynapseGroup::setPSTargetVar() sets the additional input variable (or standard "Isyn") on the postsynaptic neuron population where input from this synapse group is routed (see section \ref neuron_additional_input).
\end_toggle
\add_toggle_python
The pygenn.GeNNModel.add_synapse_population function returns a pygenn.SynapseGroup object which can be further configured, namely with:
- pygenn.SynapseGroup.ps_target_var sets the additional input variable (or standard "Isyn") on the postsynaptic neuron population where input from this synapse group is routed (see section \ref neuron_additional_input).
\end_toggle

\note
If the synapse matrix uses one of the "GLOBALG" types then the global
value of the synapse parameters are taken from the initial value provided
in `weightVarInitialisers` therefore these must be constant rather than sampled from a distribution etc.

\section defining_current_source Defining current sources
Current sources are added with the function
\add_toggle_code_cpp
model.addCurrentSource<CurrentSourceModel>(currentSourceName, targetNeuronGroupName, 
                                           paramValues, varInitialisers);
\end_toggle_code
\add_toggle_code_python
model.add_current_source(cs_name, current_source_model, pop,
                         param_space, var_space)
\end_toggle_code
where the arguments are
\arg \add_cpp_python_text{`CurrentSourceModel`: Template parameter specifying the type of current source model. These should be derived off CurrentSourceModels::Base and can either be one of the standard models or user-defined,`current_source_model`: The type of current source model. This should either be a string containing the name of a built in model or user-defined current source model returned by pygenn.genn_model.create_custom_current_source_class} (see \ref sect_own_current_source).
\arg \add_cpp_python_text{`const string &currentSourceName`,`cs_name`}: The name of the current source
\arg \add_cpp_python_text{`const string &targetNeuronGroupName`: Name of the target neuron group,`pop`: Population into which the current source should be injected (either name or NeuronGroup object)}
\arg \add_cpp_python_text{`CurrentSourceModel::ParamValues paramValues`: The, `param_space`: Dictionary containing the} parameter values (common to all current sources in the population) for the current source model. 
\arg \add_cpp_python_text{``CurrentSourceModel::VarValues varInitialisers`: The, `var_space`: Dictionary containing the} initial values or initialisation snippets for the current source model's state variables (see \ref sectVariableInitialisation)
\note
The number of current sources in a population always equals the number of neurons in the population it injects current into.

\section defining_custom_updates Defining custom updates
Custom updates are added with the function
\add_toggle_code_cpp
model.addCustomUpdate<CustomUpdateModel>(name, updateGroupName,
                                         paramValues, varInitialisers, varReferences)
\end_toggle_code
\add_toggle_code_python
model.add_custom_update(cu_name, group_name, custom_update_model,
                        param_space, var_space, var_ref_space)
\end_toggle_code
where the arguments are
\arg \add_cpp_python_text{`CustomUpdateModel`: Template parameter specifying the type of custom update model. These should be derived off CustomUpdateModels::Base and can either be one of the standard models or user-defined,`custom_update_model`: The type of custom update model. This should either be a string containing the name of a built in model or user-defined custom update model returned by pygenn.genn_model.create_custom_custom_update_class} (see \ref sect_own_custom_update).
\arg \add_cpp_python_text{`const string &name`,`cu_name`}: The name of the custom update
\arg \add_cpp_python_text{`const string &updateGroupName`,`group_name`}: The name of the group this custom update belongs to. Updates in each group can be launched as described in \ref Simulate.
\arg \add_cpp_python_text{`CustomUpdateModel::ParamValues paramValues`: The, `param_space`: Dictionary containing the} parameter values (common to all custom updates in the population) for the custom update model. 
\arg \add_cpp_python_text{`CustomUpdateModel::VarValues varInitialisers`: The, `var_space`: Dictionary containing the} initial values or initialisation snippets for the custom update model's state variables (see \ref sectVariableInitialisation)
\arg \add_cpp_python_text{`CustomUpdateModel::VarValues varReferences`: The, `var_ref_space`: Dictionary containing the} variable references for the custom update model (see \ref sectVariableReferences)

\section batching Batching
When running models on a GPU, smaller models may not fully occupy the device.
In some scenerios such as gradient-based training and parameter sweeping, this can be overcome by runing multiple copies of the same model at the same time (batching in Machine Learning speak).
Batching can be enabled on a GeNN model with:
\add_toggle_code_cpp
model.setBatchSize(512);
\end_toggle_code
\add_toggle_code_python
model.batch_size = 512
\end_toggle_code
Model parameters and sparse connectivity are shared across all batches. 
Read-write state variables are duplicated for each batch and, by default, read-only state variables are shared across all batches (see section \ref sectNeuronModels for more details).

-----
\link UserManual Previous\endlink | \link sectDefiningNetwork Top\endlink | \link sectNeuronModels Next\endlink
*/


//----------------------------------------------------------------------------
/*! 
\page sectNeuronModels Neuron models
There is a number of predefined models which can be used with the \add_cpp_python_text{ModelSpec::addNeuronPopulation,pygenn.GeNNModel.add_neuron_population} method:
- NeuronModels::Izhikevich
- NeuronModels::IzhikevichVariable
- NeuronModels::LIF
- NeuronModels::Poisson
- NeuronModels::PoissonNew
- NeuronModels::RulkovMap
- NeuronModels::SpikeSource
- NeuronModels::SpikeSourceArray
- NeuronModels::TraubMiles
- NeuronModels::TraubMilesAlt
- NeuronModels::TraubMilesFast
- NeuronModels::TraubMilesNStep

\add_toggle_python
In Python, these models can be selected by their unqualified name e.g. "Izhikevich".
\end_toggle

Example usage:
\add_toggle_code_cpp
// Izhikevich model parameters - tonic spiking
NeuronModels::Izhikevich::ParamValues izh_p(
    0.02,       // 0 - a
    0.2,        // 1 - b
    -65,        // 2 - c
    6           // 3 - d
); 

// Izhikevich model initial conditions - tonic spiking
NeuronModels::Izhikevich::VarValues izh_ini(
    -65,        // 0 - V
    -20         // 1 - U
);
    
model.addNeuronPopulation<NeuronModels::Izhikevich>("Izh1", 10, izh_p, izh_ini);
\end_toggle_code

\add_toggle_code_python
# Izhikevich model parameters - tonic spiking
izh_p= {
    "a": 0.02,
    "b": 0.2,
    "c": -65.0,
    "d": 6.0
}

# Izhikevich model initial conditions - tonic spiking
izh_ini= {
    "V": -65.0,
    "U": -20.0
}
     
pop1= model.add_neuron_population("pop1", 10, "Izhikevich", izh_p, izh_ini) 
\end_toggle_code

\section sect_own Defining your own neuron type 
In order to define a new neuron type for use in a GeNN application,
it is necessary to define a new class derived from NeuronModels::Base.
For convenience, \add_cpp_python_text{the methods this class should implement can be implemented using macros,this can be done using the ``pygenn.genn_model.create_custom_neuron_class`` function with the following basic keyword arguments}:
- \add_cpp_python_text{DECLARE_MODEL(TYPE\, NUM_PARAMS\, NUM_VARS): declared the boilerplate code required for the model e.g. the correct  specialisations of NewModels::ValueBase used to wrap the neuron model parameters and values, `class_name`: the name of the new model}.
- \add_cpp_python_text{SET_SIM_CODE(SIM_CODE),`sim_code=SIM_CODE`}: where SIM_CODE contains the code for executing 
  the integration of the model for one time stepWithin this code string, variables need to be
  referred to by \$(NAME), where NAME is the name of the variable as
  defined in the vector varNames. The code may refer to the predefined
  primitives `DT` for the
  time step size and `$(Isyn)` for the total incoming synaptic current. It can also refer to a unique ID (within the population) using $(id).
- \add_cpp_python_text{SET_THRESHOLD_CONDITION_CODE(THRESHOLD_CONDITION_CODE),`threshold_condition_code=THRESHOLD_CONDITION_CODE`} defines the condition for true spike detection.
- \add_cpp_python_text{SET_RESET_CODE(RESET_CODE),`reset_code=RESET_CODE`} defines code to be run after a true spike is emitted.
- \add_cpp_python_text{SET_PARAM_NAMES(),`param_names`} defines the names of the model parameters. 
    If defined as `NAME` here, they can then be referenced as \$(NAME) in the code string. 
    \add_toggle_cpp The length of this list should match the NUM_PARAM specified in DECLARE_MODEL. \end_toggle
    Parameters are assumed to be always of type double.
- \add_cpp_python_text{SET_VARS(),`var_name_types`} defines the names, type strings (e.g. "float", "double", etc) and (optionally) access mode
    of the neuron state variables. The type string "scalar" can be used for variables which should be implemented using the precision set globally for the model \add_cpp_python_text{with ModelSpec::setPrecision, from ``pygenn.GeNNModel.__init__``}.
    The variables defined here as `NAME` can then be used in the
    syntax \$(NAME) in the code string. If the access mode is set to \add_cpp_python_text{``VarAccess::READ_ONLY``,``VarAccess_READ_ONLY``}, GeNN applies additional optimisations and models should not write to it.
    By default such read-only variables are shared across all batches (see section \ref batching). 
    If, instead, a read-only variable should be duplicated across batches, its access mode should be set to \add_cpp_python_text{``VarAccess::READ_ONLY_DUPLICATE``,``VarAccess_READ_ONLY_DUPLICATE``}.
- \add_cpp_python_text{SET_NEEDS_AUTO_REFRACTORY(), `is_auto_refractory_required`} defines whether the neuron should include an automatic refractory period to prevent it emitting spikes in successive timesteps.

For example, we can define a leaky integrator \f$\tau\frac{dV}{dt}= -V + I_{{\rm syn}}\f$ solved using Euler's method:

\add_toggle_code_cpp
class LeakyIntegrator : public NeuronModels::Base
{
public:
    DECLARE_MODEL(LeakyIntegrator, 1, 1);
    
    SET_SIM_CODE("$(V)+= (-$(V)+$(Isyn))*(DT/$(tau));");
    SET_THRESHOLD_CONDITION_CODE("$(V) >= 1.0");
    SET_RESET_CODE("$(V) = 0.0;");
    
    SET_PARAM_NAMES({"tau"});
    
    SET_VARS({{"V", "scalar", VarAccess::READ_WRITE}});
};
IMPLEMENT_MODEL(LeakyIntegrator);
\end_toggle_code
\add_toggle_code_python
leaky_integrator_model = genn_model.create_custom_neuron_class(
    "leaky_integrator",
    
    sim_code="$(V)+= (-$(V)+$(Isyn))*(DT/$(tau));",
    threshold_condition_code="$(V) >= 1.0",
    reset_code="$(V) = 0.0;",
    
    param_names=["tau"],
    var_name_types=[("V", "scalar", VarAccess_READ_WRITE)])
\end_toggle_code

\subsection neuron_derived_params Derived Parameters
"Derived parameters" are a mechanism for enhanced efficiency when running neuron models. If parameters with model-side meaning, such as time
constants or conductances always appear in a certain combination in the model, then it is more efficient 
to pre-compute this combination and define it as a dependent parameter.

For example, because the equation defining the previous leaky integrator example has an algebraic solution, it can be more accurately solved as follows - using a derived parameter to calculate \f$\exp\left(\frac{-t}{\tau}\right)\f$:
\add_toggle_code_cpp
class LeakyIntegrator2 : public NeuronModels::Base
{
public:
    DECLARE_MODEL(LeakyIntegrator2, 1, 1);

    SET_SIM_CODE("$(V) = $(Isyn) - $(ExpTC)*($(Isyn) - $(V));");
    SET_THRESHOLD_CONDITION_CODE("$(V) >= 1.0");
    SET_RESET_CODE("$(V) = 0.0;");

    SET_PARAM_NAMES({"tau"});
    SET_VARS({{"V", "scalar", VarAccess::READ_WRITE}});
    SET_DERIVED_PARAMS({
        {"ExpTC", [](const vector<double> &pars, double dt){ return std::exp(-dt / pars[0]); }}});
};
IMPLEMENT_MODEL(LeakyIntegrator2);
\end_toggle_code
\add_toggle_code_python
leaky_integrator_2_model = genn_model.create_custom_neuron_class(
    "leaky_integrator_2",

    sim_code="$(V) = $(Isyn) - $(ExpTC)*($(Isyn) - $(V));",
    threshold_condition_code="$(V) >= 1.0",
    reset_code="$(V) = 0.0;",

    param_names=["tau"],
    var_name_types=[("V", "scalar", VarAccess_READ_WRITE)],
    derived_params=[("ExpTC", genn_model.create_dpf_class(lambda pars, dt: np.exp(-dt / pars[0]))())])
\end_toggle_code
GeNN provides several additional features that might be useful when defining more complex neuron models.

\subsection neuron_support_code Support code
Support code enables a code block to be defined that contains supporting code that will be utilized in multiple pieces of user code. Typically, these are functions that are needed in the sim code or threshold condition code. If possible, these functions should be prefixed with ``SUPPORT_CODE_FUNC`` so that both GPU and CPU versions of GeNN code have an appropriate support code function available. The support code is protected with a namespace so that it is exclusively available for the neuron population whose neurons define it.
Support code is added to a model using the \add_cpp_python_text{``SET_SUPPORT_CODE()`` macro, ``support_code`` keyword argument}, for example:
\add_toggle_code_cpp
SET_SUPPORT_CODE("SUPPORT_CODE_FUNC scalar mysin(float x){ return sin(x); }");
\end_toggle_code
\add_toggle_code_python
support_code="SUPPORT_CODE_FUNC scalar mysin(float x){ return sin(x); }"
\end_toggle_code

\subsection neuron_extra_global_param Extra global parameters
Extra global parameters are parameters common to all neurons in the population. However, unlike the standard neuron parameters, they can be varied at runtime meaning they could, for example, be used to provide a global reward signal.
These parameters are defined by using the \add_cpp_python_text{SET_EXTRA_GLOBAL_PARAMS() macro, ``extra_global_params`` keyword argument} to specify a list of variable names and type strings (like the \add_cpp_python_text{SET_VARS() macro, ``var_name_types`` keyword argument}). For example:
\add_toggle_code_cpp
SET_EXTRA_GLOBAL_PARAMS({{"R", "float"}});
\end_toggle_code
\add_toggle_code_python
extra_global_params=[("R", "float")]
\end_toggle_code
These variables are available to all neurons in the population. They can also be used in synaptic code snippets; in this case it need to be addressed with a `_pre` or `_post` postfix. For example, if the model with the "R" parameter was used for the pre-synaptic neuron population, the weight update model of a synapse population could have simulation code like:
\add_toggle_code_cpp
SET_SIM_CODE("$(x)= $(x)+$(R_pre);");
\end_toggle_code
\add_toggle_code_python
sim_code="$(x)= $(x)+$(R_pre);"
\end_toggle_code
where we have assumed that the weight update model has a variable `x` and our synapse type will only be used in conjunction with pre-synaptic neuron populations that do have the extra global parameter `R`. If the pre-synaptic population does not have the required variable/parameter, GeNN will fail when compiling the kernels.

\subsection neuron_additional_input Additional input variables
Normally, neuron models receive the linear sum of the inputs coming from all of their synaptic inputs through the \$(inSyn) variable. 
However neuron models can define additional input variables - allowing input from different synaptic inputs to be combined non-linearly.
For example, if we wanted our leaky integrator to operate on the the product of two input currents, it could be defined as follows:
\add_toggle_code_cpp
SET_ADDITIONAL_INPUT_VARS({{"Isyn2", "scalar", 1.0}});
SET_SIM_CODE("const scalar input = $(Isyn) * $(Isyn2);\n"
             "$(V) = input - $(ExpTC)*(input - $(V));");
\end_toggle_code
\add_toggle_code_python
additional_input_vars=[("Isyn2", "scalar", 1.0)],
sim_code=
    """
    const scalar input = $(Isyn) * $(Isyn2);
    $(V) = input - $(ExpTC)*(input - $(V));
    """,
\end_toggle_code
where the \add_cpp_python_text{SET_ADDITIONAL_INPUT_VARS() macro,``additional_input_vars`` keyword argument} defines the name, type and its initial value before postsynaptic inputs are applyed (see section \ref sect_postsyn for more details).

\subsection neuron_rng Random number generation
Many neuron models have probabilistic terms, for example a source of noise or a probabilistic spiking mechanism. In GeNN this can be implemented by using the following functions in blocks of model code:
- <code>\$(gennrand_uniform)</code> returns a number drawn uniformly from the interval \f$[0.0, 1.0]\f$
- <code>\$(gennrand_normal)</code> returns a number drawn from a normal distribution with a mean of 0 and a standard deviation of 1.
- <code>\$(gennrand_exponential)</code> returns a number drawn from an exponential distribution with \f$\lambda=1\f$.
- <code>\$(gennrand_log_normal, MEAN, STDDEV)</code> returns a number drawn from a log-normal distribution with the specified mean and standard deviation.
- <code>\$(gennrand_gamma, ALPHA)</code> returns a number drawn from a gamma distribution with the specified shape.

\section neuron_pop_own_type Creating neuron populations with your own neuron type
Once defined in this way, new neuron models classes, can be used in network descriptions by referring to their type e.g.
\add_toggle_code_cpp
networkModel.addNeuronPopulation<LeakyIntegrator>("Neurons", 1, 
                                                  LeakyIntegrator::ParamValues(20.0), // tau
                                                  LeakyIntegrator::VarValues(0.0)); // V
\end_toggle_code
\add_toggle_code_python
network_model.add_neuron_population("Neurons", 1, leaky_integrator_model, {"tau": 20.0}, {"V": 0.0})
\end_toggle_code

-----
\link sectDefiningNetwork Previous\endlink | \link UserManual Top\endlink | \link sectSynapseModels Next\endlink
*/

//----------------------------------------------------------------------------
/*! 
\page sectSynapseModels Weight update models

Currently 4 predefined weight update models are available: 
- WeightUpdateModels::StaticPulse
- WeightUpdateModels::StaticPulseDendriticDelay
- WeightUpdateModels::StaticGraded
- WeightUpdateModels::PiecewiseSTDP

\add_toggle_python
In Python, these models can be selected by their unqualified name e.g. "StaticPulse".
\end_toggle

\section sect34 Defining a new weight update model

Like the neuron models discussed in \ref sect_own, new weight update models are created by defining a class derived from WeightUpdateModels::Base.
For convenience, \add_cpp_python_text{the methods a new weight update model should implement can be implemented using the following basic macros,new weight update models should be implemented using the ``pygenn.genn_model.create_custom_weight_update_class`` function with the following basic keyword arguments}:
- \add_cpp_python_text{SET_DERIVED_PARAMS()\, SET_PARAM_NAMES()\, SET_VARS() and SET_EXTRA_GLOBAL_PARAMS(), `derived_params`\, `param_names`\, `var_name_types` and `extra_global_params`} perform the same roles as they do in the neuron
  models discussed in \ref sect_own.
- \add_cpp_python_text{DECLARE_WEIGHT_UPDATE_MODEL(TYPE\, NUM_PARAMS\, NUM_VARS\, NUM_PRE_VARS\, NUM_POST_VARS) is an extended version of ``DECLARE_MODEL()`` which declares the boilerplate code required for a weight update model with pre and postsynaptic as well as per-synapse state variables,`class_name`: the name of the new model}.
- \add_cpp_python_text{SET_SIM_CODE(SIM_CODE),`sim_code=SIM_CODE`}: defines the simulation code that is used when a true spike is detected. The update is performed only in timesteps after a neuron in the presynaptic population has fulfilled its threshold detection condition.
Typically, spikes lead to update of synaptic variables that then lead to the activation of input into the post-synaptic neuron. Most of the time these inputs add linearly at the post-synaptic neuron. This is assumed in GeNN and the term to be added to the activation of the post-synaptic neuron should be applied using the the \$(addToInSyn, weight) function. 
For example
\add_toggle_code_cpp
SET_SIM_CODE(
    "$(addToInSyn, $(inc));"
\end_toggle_code
\add_toggle_code_python
sim_code="$(addToInSyn, $(inc));"
\end_toggle_code
where "inc" is the increment of the synaptic input to a post-synaptic neuron for each pre-synaptic spike. The simulation code also typically contains updates to the internal synapse variables that may have contributed to \$(inc). For an example, see WeightUpdateModels::StaticPulse for a simple synapse update model and WeightUpdateModels::PiecewiseSTDP for a more complicated model that uses STDP. 
To apply input to the post-synaptic neuron with a dendritic (i.e. between the synapse and the postsynaptic neuron) delay you can instead use the \$(addToInSynDelay, weight, delay) function.
For example
\add_toggle_code_cpp
SET_SIM_CODE(
    "$(addToInSynDelay, $(inc), $(delay));");
\end_toggle_code
\add_toggle_code_python
sim_code="$(addToInSynDelay, $(inc), $(delay));"
\end_toggle_code
where, once again, `inc` is the magnitude of the input step to apply and `delay` is the length of the dendritic delay in timesteps. By implementing `delay` as a weight update model variable, heterogeneous synaptic delays can be implemented. For an example, see WeightUpdateModels::StaticPulseDendriticDelay for a simple synapse update model with heterogeneous dendritic delays. 
\note
When using dendritic delays, the <b>maximum</b> dendritic delay for a synapse populations must be specified using the `SynapseGroup::setMaxDendriticDelayTimesteps()` function.

One can also define synaptic effects that occur in the reverse direction, i.e. terms that are added to a target variable in the persynaptic neuron using the \$(addToPre, expression) function. For instance,
\add_toggle_code_cpp
SET_SIM_CODE(
    "$(addToPre, $(inc)*$(V_post));");
\end_toggle_code
\add_toggle_code_python
sim_code="$(addToPre, $(inc)*$(V_post));"
\end_toggle_code
would add terms \$(inc)*\$(V_post) to the predefined presynaptic variable \$(Isyn) for each outgoing synapse of a presynaptic neuron. One can also set alternative input variables in the presynaptic neuron as the target variable of this reverse input using \add_cpp_python_text{SynapseGroup::setPreTargetVar(),`pygenn.SynapseGroup.pre_target_var`}, see section \ref neuron_additional_input on how to define additional input variables for a neuron population.
\note
\$(addToPre, expression) can be used in \add_cpp_python_text{SET_SIM_CODE(),`sim_code`}, \add_cpp_python_text{SET_EVENT_CODE(),`event_code`}, \add_cpp_python_text{SET_SYNAPSE_DYNAMICS_CODE(),`synapse_dynamics_code`}, \add_cpp_python_text{SET_LEARN_POST_CODE(),`learn_post_code`}.
\note
Unlike for normal forward synaptic actions, reverse synaptic actions with \$(addToPre,\$(inc)) are not modulated through a post-synaptic model but added directly into the indicated presynaptic target input variable, such as \$(Isyn).

- \add_cpp_python_text{SET_LEARN_POST_CODE(LEARN_POST_CODE),`learn_post_code=LEARN_POST_CODE`} defines the code which is used in the learnSynapsesPost kernel/function, which performs updates to synapses that are triggered by post-synaptic spikes. This is typically used in STDP-like models e.g. WeightUpdateModels::PiecewiseSTDP.
- \add_cpp_python_text{SET_NEEDS_PRE_SPIKE_TIME(PRE_SPIKE_TIME_REQUIRED) and SET_NEEDS_POST_SPIKE_TIME(POST_SPIKE_TIME_REQUIRED),`is_pre_spike_time_required=PRE_SPIKE_TIME_REQUIRED` and `is_post_spike_time_required=POST_SPIKE_TIME_REQUIRED`} define whether the weight update needs to know the times of the spikes emitted from the pre and postsynaptic populations. These can then be accessed through \$(sT_pre) and \$(sT_post). For example an STDP rule would be likely to require:
\add_toggle_code_cpp
SET_NEEDS_PRE_SPIKE_TIME(true);
SET_NEEDS_POST_SPIKE_TIME(true);
\end_toggle_code
\add_toggle_code_python
is_pre_spike_time_required=True, is_post_spike_time_required=True
\end_toggle_code
- \add_cpp_python_text{SET_NEEDS_PREV_PRE_SPIKE_TIME(PREV_PRE_SPIKE_TIME_REQUIRED) and SET_NEEDS_PREV_POST_SPIKE_TIME(PREV_POST_SPIKE_TIME_REQUIRED),`is_prev_pre_spike_time_required=PREV_PRE_SPIKE_TIME_REQUIRED` and `is_prev_post_spike_time_required=PREV_POST_SPIKE_TIME_REQUIRED`} define whether the weight update needs to know the times of the previous (i.e. not the ones being processed this timeste) spikes emitted from the pre and postsynaptic populations. These can then be accessed through \$(prev_sT_pre) and \$(prev_sT_post).

For example, we can define a simple additive STDP rule with nearest-neighbour spike pairing and the following time-dependence:
\f{align*}{
  \Delta w_{ij} & = \
    \begin{cases}
      A_{+}\exp\left(-\frac{\Delta t}{\tau_{+}}\right) & if\, \Delta t>0\\
      A_{-}\exp\left(\frac{\Delta t}{\tau_{-}}\right) & if\, \Delta t\leq0
    \end{cases}
\f}
in a fully event-driven manner as follows:
\add_toggle_code_cpp
class STDPAdditive : public WeightUpdateModels::Base
{
public:
    DECLARE_WEIGHT_UPDATE_MODEL(STDPAdditive, 6, 1, 0, 0);

    SET_PARAM_NAMES({"tauPlus", "tauMinus", "Aplus", "Aminus",
                     "Wmin", "Wmax"});
    SET_VARS({{"g", "scalar"}});

    SET_SIM_CODE(
        "$(addToInSyn, $(g));\n"
        "const scalar dt = $(t) - $(sT_post); \n"
        "if (dt > 0) {\n"
        "    const scalar timing = exp(-dt / $(tauMinus));\n"
        "    const scalar newWeight = $(g) - ($(Aminus) * timing);\n"
        "    $(g) = fmax($(Wmin), fmin($(Wmax), newWeight));\n"
        "}\n");
    SET_LEARN_POST_CODE(
        "const scalar dt = $(t) - $(sT_pre);\n"
        "if (dt > 0) {\n"
        "    const scalar timing = exp(-dt / $(tauPlus));\n"
        "    const scalar newWeight = $(g) + ($(Aplus) * timing);\n"
        "    $(g) = fmax($(Wmin), fmin($(Wmax), newWeight));\n"
        "}\n");

    SET_NEEDS_PRE_SPIKE_TIME(true);
    SET_NEEDS_POST_SPIKE_TIME(true);
};
IMPLEMENT_MODEL(STDPAdditive);
\end_toggle_code
\add_toggle_code_python
stdp_additive_model = genn_model.create_custom_weight_update_class(
    "stdp_additive",
    param_names=["tauPlus", "tauMinus", "aPlus", "aMinus", "wMin", "wMax"],
    var_name_types=[("g", "scalar")],

    sim_code="""
        $(addToInSyn, $(g));
        const scalar dt = $(t) - $(sT_post);
        if (dt > 0) {
            const scalar timing = exp(-dt / $(tauMinus));
            const scalar newWeight = $(g) - ($(Aminus) * timing);
            $(g) = fmax($(Wmin), fmin($(Wmax), newWeight));
        }
        """,
    learn_post_code="""
        const scalar dt = $(t) - $(sT_pre);
        if (dt > 0) {
            const scalar timing = exp(-dt / $(tauPlus));
            const scalar newWeight = $(g) + ($(Aplus) * timing);
            $(g) = fmax($(Wmin), fmin($(Wmax), newWeight));
        }
        """,

    is_pre_spike_time_required=True,
    is_post_spike_time_required=True)
\end_toggle_code

\subsection wum_pre_post_dynamics Pre and postsynaptic dynamics
The memory required for synapse variables and the computational cost of updating them tends to grow with \f$O(N^2)\f$ with the number of neurons.
Therefore, if it is possible, implementing synapse variables on a per-neuron rather than per-synapse basis is a good idea. 
The \add_cpp_python_text{SET_PRE_VARS() and SET_POST_VARS() macros, `pre_var_name_types` and `post_var_name_types` keyword arguments} are used to define any pre or postsynaptic state variables.
Then the \add_cpp_python_text{SET_PRE_DYNAMICS_CODE() and SET_POST_DYNAMICS_CODE() macros, `pre_dynamics_code` and `post_dynamics_code` keyword arguments} can be used to define any time-driven updates to these variables and the \add_cpp_python_text{SET_PRE_SPIKE_CODE() and SET_POST_SPIKE_CODE() macros, `pre_spike_code` and `post_spike_code` keyword arguments} any spike-driven updates.
For example, using pre and postsynaptic variables, our event-driven STDP rule can be extended to use all-to-all spike pairing using pre and postsynaptic <i>trace</i> variables \cite Morrison2008 :
\note
These pre and postsynaptic code snippets can only access the corresponding pre and postsynaptic variables as well as those associated with the pre or postsynaptic neuron population. Like other state variables, variables defined here as `NAME` can be accessed in weight update model code strings using the \$(NAME) syntax. 

\add_toggle_code_cpp
class STDPAdditive2 : public WeightUpdateModels::Base
{
public:
    DECLARE_WEIGHT_UPDATE_MODEL(STDPAdditive2, 6, 1, 1, 1);

    SET_PARAM_NAMES({"tauPlus", "tauMinus", "Aplus", "Aminus",
                     "Wmin", "Wmax"});
    SET_DERIVED_PARAMS({
        {"tauPlusDecay", [](const std::vector<double> &pars, double dt){ return std::exp(-dt / pars[0]); }},
        {"tauMinusDecay", [](const std::vector<double> &pars, double dt){ return std::exp(-dt / pars[1]); }}});
    SET_VARS({{"g", "scalar"}});
    SET_PRE_VARS({{"preTrace", "scalar"}});
    SET_POST_VARS({{"postTrace", "scalar"}});
    
    SET_SIM_CODE(
        "$(addToInSyn, $(g));\n"
        "const scalar dt = $(t) - $(sT_post); \n"
        "if (dt > 0) {\n"
        "    const scalar newWeight = $(g) - ($(Aminus) * $(postTrace));\n"
        "    $(g) = fmax($(Wmin), fmin($(Wmax), newWeight));\n"
        "}\n");
    SET_LEARN_POST_CODE(
        "const scalar dt = $(t) - $(sT_pre);\n"
        "if (dt > 0) {\n"
        "    const scalar newWeight = $(g) + ($(Aplus) * $(preTrace));\n"
        "    $(g) = fmax($(Wmin), fmin($(Wmax), newWeight));\n"
        "}\n");
    SET_PRE_SPIKE_CODE("$(preTrace) += 1.0;\n");
    SET_POST_SPIKE_CODE("$(postTrace) += 1.0;\n");
    SET_PRE_DYNAMICS_CODE("$(preTrace) *= $(tauPlusDecay);\n");
    SET_POST_DYNAMICS_CODE("$(postTrace) *= $(tauMinusDecay);\n");
    
    SET_NEEDS_PRE_SPIKE_TIME(true);
    SET_NEEDS_POST_SPIKE_TIME(true);
};
IMPLEMENT_MODEL(STDPAdditive2);
\end_toggle_code
\add_toggle_code_python
stdp_additive_2_model = genn_model.create_custom_weight_update_class(
    "stdp_additive_2",
    param_names=["tauPlus", "tauMinus", "aPlus", "aMinus", "wMin", "wMax"],
    var_name_types=[("g", "scalar")],
    pre_var_name_types=[("preTrace", "scalar")],
    post_var_name_types=[("postTrace", "scalar")],
    
    sim_code="""
        $(addToInSyn, $(g));
        const scalar dt = $(t) - $(sT_post);
        if(dt > 0) {
            const scalar newWeight = $(g) - ($(aMinus) * $(postTrace));
            $(g) = fmin($(wMax), fmax($(wMin), newWeight));
        }
        """,
    learn_post_code="""
        const scalar dt = $(t) - $(sT_pre);
        if(dt > 0) {
            const scalar newWeight = $(g) + ($(aPlus) * $(preTrace));
            $(g) = fmin($(wMax), fmax($(wMin), newWeight));
        }
        """,

    pre_spike_code="""
        $(preTrace) += 1.0;
        """,
    pre_dynamics_code="""
        $(preTrace) *= $(tauPlusDecay);
        """,
    post_spike_code="""
        $(postTrace) += 1.0;
        """,
    post_dynamics_code="""
        $(postTrace) *= $(tauMinusDecay);
        """,
    
    is_pre_spike_time_required=True,
    is_post_spike_time_required=True)
\end_toggle_code

\subsection wum_synapse_dynamics Synapse dynamics
Unlike the event-driven updates previously described, synapse dynamics code is run for each synapse, each timestep i.e. unlike the others it is time-driven. 
This can be used where synapses have internal variables and dynamics that are described in continuous time, e.g. by ODEs.
However using this mechanism is typically computationally very costly because of the large number of synapses in a typical network. 
By using the \$(addToInSyn) and \$(addToInSynDelay) functions discussed in the context of SET_SIM_CODE(), the synapse dynamics can also be used to implement continuous synapses for rate-based models.
Synapse dynamics code is added to a model using the \add_cpp_python_text{``SET_SYNAPSE_DYNAMICS_CODE()`` macro, ``synapse_dynamics_code`` keyword argument}, for example a continous synapse could be implemented as follows:
\add_toggle_code_cpp
SET_SYNAPSE_DYNAMICS_CODE("$(addToInSyn, $(g) * $(V_pre));");
\end_toggle_code
\add_toggle_code_python
synapse_dynamics_code="$(addToInSyn, $(g) * $(V_pre));",
\end_toggle_code

\subsection wum_spike_like_events Spike-like events
As well as time-driven synapse dynamics and spike event-driven updates, GeNN weight update models also support "spike-like events". These are triggerd by a threshold condition -- implemented with the code string specified using the  \add_cpp_python_text{SET_EVENT_THRESHOLD_CONDITION_CODE() macro,`event_threshold_condition_code` keyword argument}. This typically involves the pre-synaptic variables, e.g. the membrane potential: 
\add_toggle_code_cpp
SET_EVENT_THRESHOLD_CONDITION_CODE("$(V_pre) > -0.02");
\end_toggle_code
\add_toggle_code_python
event_threshold_condition_code="$(V_pre) > -0.02"
\end_toggle_code
Whenever this expression evaluates to true, the event code set using the \add_cpp_python_text{SET_EVENT_CODE() macro,`event_code` keyword argument} is executed. For an example, see WeightUpdateModels::StaticGraded.
Weight update models can indicate whether they require the times of these spike-like-events using the \add_cpp_python_text{SET_NEEDS_PRE_SPIKE_EVENT_TIME() and SET_NEEDS_PREV_PRE_SPIKE_EVENT_TIME() macros, ``is_pre_spike_event_time_required`` and ``is_prev_pre_spike_event_time_required`` keyword arguments}.
These times can then be accessed through the \$(seT_pre) and \$(prev_seT_pre) variables.
-----
\link sectNeuronModels Previous\endlink | \link UserManual Top\endlink | \link sect_postsyn Next\endlink
*/

//----------------------------------------------------------------------------
/*! 

\page sect_postsyn Postsynaptic integration methods

There are currently 3 built-in postsynaptic integration methods:
- PostsynapticModels::ExpCurr
- PostsynapticModels::ExpCond
- PostsynapticModels::DeltaCurr

\add_toggle_python
In Python, these models can be selected by their unqualified name e.g. "ExpCurr".
\end_toggle

\section sect_new_postsynaptic Defining a new postsynaptic model
The postsynaptic model defines how synaptic activation translates into an input current (or other input term for models that are not current based). It also can contain equations defining dynamics that are applied to the (summed) synaptic activation, e.g. an exponential decay over time.
In the same manner as to both the neuron and weight update models discussed in \ref sect_own and \ref sect34, postsynamic model definitions are encapsulated in a class derived from PostsynapticModels::Base. Again, the methods that a postsynaptic model should implement can be implemented using the following \add_cpp_python_text{macros, keyword arguments}:
- \add_cpp_python_text{DECLARE_MODEL(TYPE\, NUM_PARAMS\, NUM_VARS)\, SET_DERIVED_PARAMS()\, SET_PARAM_NAMES()\, SET_VARS(), `param_names`\, `var_name_types` and `derived_params`}  perform the same roles as they do in the neuron models discussed in \ref sect_own.
- \add_cpp_python_text{SET_DECAY_CODE(DECAY_CODE),decay_code=DECAY_CODE} defines the code which provides the continuous time dynamics for the summed presynaptic inputs to the postsynaptic neuron. This usually consists of some kind of decay function.
- \add_cpp_python_text{SET_APPLY_INPUT_CODE(APPLY_INPUT_CODE),apply_input_code=APPLY_INPUT_CODE} defines the code specifying the conversion from synaptic inputs to a postsynaptic neuron input current. e.g. for a conductance model:
\add_toggle_code_cpp
SET_APPLY_INPUT_CODE("$(Isyn) += $(inSyn) * ($(E) - $(V))");
\end_toggle_code
\add_toggle_code_python
apply_input_code="$(Isyn) += $(inSyn) * ($(E) - $(V))"
\end_toggle_code
where \$(E) is a postsynaptic model parameter specifying reversal potential and \$(V) is the variable containing the postsynaptic neuron's membrane potential.
As discussed in \ref predefinedVars, \$(Isyn) is the built in variable used to sum neuron input.
However additional input variables can be added to a neuron model using the \add_cpp_python_text{SET_ADDITIONAL_INPUT_VARS() macro,`additional_input_vars` keyword argument} (see \ref sect_own for more details).


-----
\link sectNeuronModels Previous\endlink | \link UserManual Top\endlink | \link sectCurrentSourceModels Next\endlink
*/

//----------------------------------------------------------------------------
/*! 
\page sectCurrentSourceModels Current source models
There is a number of predefined models which can be used with the ModelSpec::addCurrentSource function:
- CurrentSourceModels::DC
- CurrentSourceModels::GaussianNoise
- CurrentSourceModels::PoissonExp

\add_toggle_python
In Python, these models can be selected by their unqualified name e.g. "DC".
\end_toggle

\section sect_own_current_source Defining your own current source model 

In order to define a new current source type for use in a GeNN application,
it is necessary to define a new class derived from CurrentSourceModels::Base. 
For convenience the methods this class should implement can be implemented using \add_cpp_python_text{macros, keyword arguments}:
- \add_cpp_python_text{DECLARE_MODEL(TYPE\, NUM_PARAMS\, NUM_VARS)\, SET_DERIVED_PARAMS()\, SET_PARAM_NAMES()\, SET_VARS(), ``derived_params``\, ``param_names``\, ``var_name_types``} perform the same roles as they do in the neuron models discussed in \ref sect_own.
- \add_cpp_python_text{SET_INJECTION_CODE(INJECTION_CODE),``injection_code=INJECTION_CODE``}: where INJECTION_CODE contains the code for injecting current into the neuron every simulation timestep. The \$(injectCurrent, $(amp)) function is used to inject current.

For example, using these \add_cpp_python_text{macros,keyword arguments}, we can define a uniformly distributed noisy current source:
\add_toggle_code_cpp
class UniformNoise : public CurrentSourceModels::Base
{
public:
    DECLARE_MODEL(UniformNoise, 1, 0);

    SET_INJECTION_CODE("$(injectCurrent, $(gennrand_uniform) * $(magnitude));");

    SET_PARAM_NAMES({"magnitude"});
};
\end_toggle_code
\add_toggle_code_python
uniform_noise_model = genn_model.create_custom_current_source_class(
    "uniform_noise",

    param_names=["magnitude"],
    injection_code="$(injectCurrent, $(gennrand_uniform) * $(magnitude));")
\end_toggle_code

-----
\link sect_postsyn Previous\endlink | \link UserManual Top\endlink | \link sectCustomUpdate Next\endlink
*/

//----------------------------------------------------------------------------
/*! 
\page sectCustomUpdate Custom update models
The neuron groups, synapse groups and current sources described in previous sections are all updated automatically every timestep. However, in many types of model, there are also processes that would benefit from GPU acceleration but only need to be triggered occasionally. For example, such updates could be used in a classifier to to reset the state of neurons after a stimuli has been presented or in a model which uses gradient-based learning to optimize network weights based on gradients accumulated over several timesteps.

Custom updates allows such updates to be described as models, similar to the neuron and synapse models described in the preceding sections. The custom update system also provides functionality for efficiently calculating the tranpose of variables associated with synapse groups with SynapseMatrixType::DENSE_INDIVIDUALG connectivity. The predefined CustomUpdateModels::Transpose model can be used to do just this for a synapse group with a single variable.

\section sect_own_custom_update Defining your own custom update model
In order to define a new custom update model for use in a GeNN application,
it is necessary to define a new class derived from CustomUpdateModels::Base. 
For convenience the methods this class should implement can be implemented using \add_cpp_python_text{macros, keyword arguments}:
- \add_cpp_python_text{SET_DERIVED_PARAMS()\, SET_PARAM_NAMES()\, SET_VARS(), ``derived_params``\, ``param_names``\, ``var_name_types``} perform the same roles as they do in the neuron models discussed in \ref sect_own.
- \add_cpp_python_text{DECLARE_CUSTOM_UPDATE_MODEL(TYPE\, NUM_PARAMS\, NUM_VARS\, NUM_VAR_REFS) is an extended version of ``DECLARE_MODEL()`` which declares the boilerplate code required for a custom update with variable references as well as variables and parameters,`class_name`: the name of the new model}.
- \add_cpp_python_text{SET_VAR_REFS(),`var_refs`} defines the names, type strings (e.g. "float", "double", etc) and (optionally) access mode
    of the varaible references. The variables defined here as `NAME` can then be used in the syntax \$(NAME) in the update code string. Variable reference types must match those of the underlying variables. 
- \add_cpp_python_text{SET_UPDATE_CODE(UPDATE_CODE),``update_code=UPDATE_CODE``}: where UPDATE_CODE contains the code for to perform the custom update.

For example, using these \add_cpp_python_text{macros,keyword arguments}, we can define a custom update which will set a referenced variable to the value of a custom update model state variable:
\add_toggle_code_cpp
class Reset : public CustomUpdateModels::Base
{
public:
    DECLARE_CUSTOM_UPDATE_MODEL(Reset, 0, 1, 1);

    SET_UPDATE_CODE("$(r) = $(v);");

    SET_VARS({{"v", "scalar", VarAccess::READ_ONLY}});
    SET_VAR_REFS({{"r", "scalar"}, 
};
\end_toggle_code
\add_toggle_code_python
reset_model = genn_model.create_custom_custom_update_class(
    "reset",

    var_name_types=[("v", "scalar", VarAccess_READ_ONLY)],
    var_refs=[("r", "scalar")],
    update_code="$(r) = $(v);")
\end_toggle_code

\subsection custom_update_reduction Batch reduction
As well as the standard variable access modes described in \ref subsect11, custom updates support variables with several 'reduction' access modes:
- \add_cpp_python_text{VarAccess::REDUCE_BATCH_SUM, ``pygenn.genn_wrapper.Models.VarAccess_REDUCE_BATCH_SUM``}
- \add_cpp_python_text{VarAccess::REDUCE_BATCH_MAX, ``pygenn.genn_wrapper.Models.VarAccess_REDUCE_BATCH_MAX``}

These access modes allow values read from variables duplicated across batches to be reduced into variables that are shared across batches.
For example, in a gradient-based learning scenario, a model like this could be used to sum gradients from across all batches so they can be used as the input to a learning rule operating on shared synaptic weights:

\add_toggle_code_cpp
class Reset : public CustomUpdateModels::Base
{
public:
    DECLARE_CUSTOM_UPDATE_MODEL(Reset, 0, 1, 1);

    SET_UPDATE_CODE(
        "$(reducedGradient) = $(gradient);\n"
        "$(gradient) = 0;\n");

    SET_VARS({{"reducedGradient", "scalar", VarAccess::REDUCE_BATCH_SUM}});
    SET_VAR_REFS({{"gradient", "scalar"}, 
};
\end_toggle_code
\add_toggle_code_python
gradient_batch_reduce_model = genn_model.create_custom_custom_update_class(
    "gradient_batch_reduce",
    var_name_types=[("reducedGradient", "scalar", VarAccess_REDUCE_BATCH_SUM)],
    var_refs=[("gradient", "scalar")],
    update_code="""
    $(reducedGradient) = $(gradient);
    $(gradient) = 0;
    """)
\end_toggle_code
\note
Reading from variables with a reduction access mode is undefined behaviour.


-----
\link sectCurrentSourceModels Previous\endlink | \link UserManual Top\endlink | \link subsect34 Next\endlink
*/

//----------------------------------------------------------------------------
/*! 
\page subsect34 Synaptic matrix types

Synaptic matrix types are made up of two components: SynapseMatrixConnectivity and SynapseMatrixWeight.
SynapseMatrixConnectivity defines what data structure is used to store the synaptic matrix:
- SynapseMatrixConnectivity::DENSE stores synaptic matrices as a dense matrix. Large dense matrices require a large amount of memory and if they contain a lot of zeros it may be inefficient.
- SynapseMatrixConnectivity::SPARSE stores synaptic matrices in a(padded) 'ragged array' format. In general, this is less efficient to traverse using a GPU than the dense matrix format but does result in significant memory savings for large matrices.
Sparse matrix connectivity is stored using several variables whose names, like state variables, have the name of the synapse population appended to them:
        -# `const unsigned int maxRowLength`: a constant set via the ``SynapseGroup::setMaxConnections`` method which specifies the maximum number of connections in any given row (this is the width the structure is padded to). 
        -# `unsigned int *rowLength` (sized to number of presynaptic neurons): actual length of the row of connections associated with each presynaptic neuron
        -# `unsigned int *ind` (sized to ``maxRowLength * number of presynaptic neurons``): Indices of corresponding postsynaptic neurons concatenated for each presynaptic neuron.
For example, consider a network of two presynaptic neurons connected to three postsynaptic neurons: 0th presynaptic neuron connected to 1st and 2nd postsynaptic neurons, the 1st presynaptic neuron connected only to the 0th neuron. Indexing starts from 0 and X represents a padding value:
\code
maxRowLength = 2
ind = [1 2 0 X]
rowLength = [2 1]  
\endcode
Weight update model variables associated with the sparsely connected synaptic population will be kept in an array using the same indexing as ind. For example, a variable caled \c g will be kept in an array such as:
\c g=[g_Pre0-Post1 g_pre0-post2 g_pre1-post0 X]
- SynapseMatrixConnectivity::BITMASK is an alternative sparse matrix implementation where which synapses within the matrix are present is specified as a binary array (see \ref ex_mbody). This structure is somewhat less efficient than the ``SynapseMatrixConnectivity::SPARSE`` format and doesn't allow individual weights per synapse. However it does require the smallest amount of GPU memory for large networks.
- SynapseMatrixConnectivity::PROCEDURAL is a new approach where, rather than being stored in memory, connectivity described using \ref sectSparseConnectivityInitialisation is generated 'on the fly' as spikes are processed (see \cite Knight2020 for more information). Therefore, this approach offers very large memory savings for a small performance cost but does not currently support plasticity.

\add_python_text{In Python\, SynapseMatrixConnectivity::SPARSE connectivity can be manually initialised from lists of pre and postsynaptic indices using the pygenn.SynapseGroup.set_sparse_connections method.}
Furthermore the SynapseMatrixWeight defines how 
- SynapseMatrixWeight::INDIVIDUAL allows each individual synapse to have unique weight update model variables. 
Their values must be initialised at runtime and, if running on the GPU, copied across from the user side code, using the \c pushXXXXXStateToDevice function, where XXXX is the name of the synapse population.
- SynapseMatrixWeight::INDIVIDUAL_PSM allows each postsynapic neuron to have unique post synaptic model variables.
Their values must be initialised at runtime and, if running on the GPU, copied across from the user side code, using the \c pushXXXXXStateToDevice function, where XXXX is the name of the synapse population.
- SynapseMatrixWeight::GLOBAL saves memory by only maintaining one copy of the weight update model variables.
This is automatically initialized to the initial value passed to \add_cpp_python_text{ModelSpec::addSynapsePopulation, pygenn.GeNNModel.add_synapse_population}.
- SynapseMatrixWeight::PROCEDURAL generates weight update model variable values described using \ref sectVariableInitialisation 'on the fly' as spikes are processed. This is typically used alongside SynapseMatrixConnectivity::PROCEDURAL for large models with static connectivity and weights/delays sampled from probability distributions (see \cite Knight2020 for an example).

Only certain combinations of SynapseMatrixConnectivity and SynapseMatrixWeight are sensible therefore, to reduce confusion, the SynapseMatrixType enumeration defines the following options which can be passed to \add_cpp_python_text{ModelSpec::addSynapsePopulation, pygenn.GeNNModel.add_synapse_population}:
- SynapseMatrixType::SPARSE_GLOBALG
- SynapseMatrixType::SPARSE_GLOBALG_INDIVIDUAL_PSM
- SynapseMatrixType::SPARSE_INDIVIDUALG
- SynapseMatrixType::DENSE_GLOBALG
- SynapseMatrixType::DENSE_GLOBALG_INDIVIDUAL_PSM
- SynapseMatrixType::DENSE_INDIVIDUALG
- SynapseMatrixType::BITMASK_GLOBALG
- SynapseMatrixType::BITMASK_GLOBALG_INDIVIDUAL_PSM
- SynapseMatrixType::PROCEDURAL_GLOBALG
- SynapseMatrixType::PROCEDURAL_PROCEDURALG

\add_toggle_python
In Python, these matrix types can be selected by their unqualified name e.g. "DENSE_INDIVIDUALG".
\end_toggle

-----
\link sectCustomUpdate Previous\endlink | \link UserManual Top\endlink | \link sectVariableInitialisation Next\endlink
*/
//----------------------------------------------------------------------------
/*! 
\page sectVariableInitialisation Variable initialisation

Neuron, weight update, postsynaptic and custom update models all have state variables which GeNN can automatically initialise. 

Previously we have shown variables being initialised to constant values such as:
\add_toggle_code_cpp
NeuronModels::TraubMiles::VarValues ini(
    0.0529324,     // 1 - prob. for Na channel activation m
    ...
);
\end_toggle_code
\add_toggle_code_python
ini = {"m": 0.0529324, ...}
\end_toggle_code
\add_toggle_cpp
state variables can also be left _uninitialised_ leaving it up to the user code to initialise them between the calls to ``initialize()`` and ``initializeSparse()``:

\code
NeuronModels::TraubMiles::VarValues ini(
    uninitialisedVar(),     // 1 - prob. for Na channel activation m
    ...
);
\endcode
\end_toggle
\add_toggle_python
state variables can alse be manually initialised to values specified in a numpy array:
\code
ini = {"m": np.arange(400.0), ...}
\endcode
\end_toggle
or initialised using one of a number of predefined _variable initialisation snippets_:
- InitVarSnippet::Kernel
- InitVarSnippet::Uniform
- InitVarSnippet::Normal
- InitVarSnippet::NormalClipped
- InitVarSnippet::NormalClippedDelay
- InitVarSnippet::Exponential
- InitVarSnippet::Gamma

\add_toggle_python
In Python, these models can be selected by their unqualified name e.g. "Normal".
\end_toggle
For example, to initialise a parameter using values drawn from the normal distribution:
\add_toggle_code_cpp
InitVarSnippet::Normal::ParamValues params(
    0.05,   // 0 - mean
    0.01);  // 1 - standard deviation
    
NeuronModels::TraubMiles::VarValues ini(
    initVar<InitVarSnippet::Normal>(params),     // 1 - prob. for Na channel activation m
    ...
);
\end_toggle_code
\add_toggle_code_python
params = {"mean": 0.05, "sd": 0.01}
ini = {"m": genn_model.init_var("Normal", params), ...}
\end_toggle_code
\section sect_new_var_init Defining a new variable initialisation snippet
Similarly to neuron, weight update and postsynaptic models, new variable initialisation snippets can be created by simply defining a class in the model description.
For example, when initialising excitatory (positive) synaptic weights with a normal distribution they should be clipped at 0 so the long tail of the normal distribution doesn't result in negative weights.
This could be implemented using the following variable initialisation snippet which redraws until samples are within the desired bounds:
\add_toggle_code_cpp
class NormalPositive : public InitVarSnippet::Base
{
public:
    DECLARE_SNIPPET(NormalPositive, 2);

    SET_CODE(
        "scalar normal;\n"
        "do\n"
        "{\n"
        "   normal = $(mean) + ($(gennrand_normal) * $(sd));\n"
        "} while (normal < 0.0);\n"
        "$(value) = normal;\n");

    SET_PARAM_NAMES({"mean", "sd"});
};
IMPLEMENT_SNIPPET(NormalPositive);
\end_toggle_code
\add_toggle_code_python
normal_positive_model = genn_model.create_custom_init_var_snippet_class(
    "normal_positive",
    param_names=["mean", "sd],
    var_init_code=
        """
        scalar normal;
        do
        {
           normal = $(mean) + ($(gennrand_normal) * $(sd));
        } while (normal < 0.0);
        $(value) = normal;
        """)
\end_toggle_code
Within the snippet of code specified using the \add_cpp_python_text{`SET_CODE()` macro, `var_init_code` keyword argument}, when initialisising neuron and postaynaptic model state variables , the \$(id) variable can be used to access the id of the neuron being initialised.
Similarly, when initialising weight update model state variables, the \$(id_pre) and \$(id_post) variables can used to access the ids of the pre and postsynaptic neurons connected by the synapse being initialised.

\section sect_var_init_modes Variable locations
Once you have defined <b>how</b> your variables are going to be initialised you need to configure <b>where</b> they will be allocated. 
By default memory is allocated for variables on both the GPU and the host.
However, the following alternative 'variable locations' are available:
- VarLocation::DEVICE - Variables are only allocated on the GPU, saving memory but meaning that they can't easily be copied to the host - best for internal state variables.
- VarLocation::HOST_DEVICE - Variables are allocated on both the GPU and the host  - the default.
- VarLocation::HOST_DEVICE_ZERO_COPY - Variables are allocated as 'zero-copy' memory accessible to the host and GPU - useful on devices such as Jetson TX1 where physical memory is shared between the GPU and CPU.

\note
'Zero copy' memory is only supported on newer embedded systems such as the Jetson TX1 where there is no physical seperation between GPU and host memory and thus the same block of memory can be shared between them. 

These modes can be set as a model default using ``ModelSpec::setDefaultVarLocation`` or on a per-variable basis using one of the following functions:
- NeuronGroup::setSpikeLocation
- NeuronGroup::setSpikeEventLocation
- NeuronGroup::setSpikeTimeLocation
- NeuronGroup::setVarLocation
- SynapseGroup::setWUVarLocation
- SynapseGroup::setWUPreVarLocation
- SynapseGroup::setWUPostVarLocation
- SynapseGroup::setPSVarLocation
- SynapseGroup::setInSynVarLocation


-----
\link subsect34 Previous\endlink | \link UserManual Top\endlink | \link sectVariableReferences Next\endlink
*/
//----------------------------------------------------------------------------
/*! 
\page sectVariableReferences Variable references
As well as state variables, custom updates have variable references which are used to reference variables belonging to other neuron and synapse groups or even other custom updates.

\add_toggle_cpp
The variable references required by a model called SetTime could be assigned to various types of variable using the following syntax:
\code
SetTime::VarReferences neuronVarReferences(createVarRef(ng, "V"));
SetTime::VarReferences currentSourceVarReferences(createVarRef(cs, "V"));
SetTime::VarReferences customUpdateVarReferences(createVarRef(cu, "V"));
SetTime::VarReferences postsynapticModelVarReferences(createPSMVarRef(sg, "V"));
SetTime::VarReferences wuPreVarReferences(createWUPreVarRef(sg, "Pre")); 
SetTime::VarReferences wuPostVarReferences(createWUPostVarRef(sg, "Post")); 
\endcode
\end_toggle
\add_toggle_python
A variable reference called R could be assigned to various types of variable using the following syntax:
\code
neuron_var_ref =  {"R": genn_model.create_var_ref(ng, "V")}
current_source_var_ref =  {"R": genn_model.create_var_ref(cs, "V")}
custom_update_var_ref = {"R": genn_model.create_var_ref(cu, "V")}
postsynaptic_model_var_ref =  {"R": genn_model.create_psm_var_ref(sg, "V")}
wu_pre_var_ref =  {"R": genn_model.create_wu_pre_var_ref(sg, "Pre")}
wu_post_var_ref =  {"R": genn_model.create_wu_post_var_ref(sg, "Post")}
\endcode
\end_toggle
where ng is a \add_cpp_python_text{NeuronGroup pointer (as returned by ModelSpec::addNeuronPopulation),pygenn.NeuronGroup (as returned by pygenn.GeNNModel.add_neuron_population)}, cs is a \add_cpp_python_text{CurrentSource pointer (as returned by ModelSpec::addCurrentSource),pygenn.CurrentSource (as returned by pygenn.GeNNModel.add_current_source)}, cu is a \add_cpp_python_text{CustomUpdate pointer (as returned by ModelSpec::addCustomUpdate),pygenn.genn_groups.CustomUpdate (as returned by pygenn.GeNNModel.add_custom_update)} and sg is a \add_cpp_python_text{SynapseGroup pointer (as returned by ModelSpec::addSynapsePopulation),pygenn.SynapseGroup (as returned by pygenn.GeNNModel.add_synapse_population)}.

While references of these types can be used interchangably in the same custom update, as long as all referenced variables have the same delays and belong to populations of the same size, per-synapse weight update model variables must be referenced with slightly different syntax:
\add_toggle_code_cpp
SetTime::WUVarReferences wuVarReferences(createWUVarRef(sg, "g"));
SetTime::WUVarReferences cuWUVarReferences(createWUVarRef(cu, "g"));
\end_toggle_code
\add_toggle_code_python
wu_var_ref = {"R": create_wu_var_ref(sg, "g")}
cu_wu_var_ref = {"R": create_wu_var_ref(cu, "g")}
\end_toggle_code
where sg is a \add_cpp_python_text{SynapseGroup pointer (as returned by ModelSpec::addSynapsePopulation),pygenn.SynapseGroup (as returned by pygenn.GeNNModel.add_synapse_population)} and cu is a \add_cpp_python_text{CustomUpdate pointer (as returned by ModelSpec::addCustomUpdate),pygenn.genn_groups.CustomUpdate (as returned by pygenn.GeNNModel.add_custom_update)} which operates on another synapse group's state variables.

These 'weight update variable references' also have the additional feature that they can be used to define a link to a 'transpose' variable:
\add_toggle_code_cpp
SetTime::WUVarReferences wuTransposeVarReferences(createWUVarRef(sg, "g", backSG, "g"));
\end_toggle_code
\add_toggle_code_python
wu_transpose_var_ref = {"R": create_wu_var_ref(sg, "g", back_sg, "g")}
\end_toggle_code
where \add_cpp_python_text{backSG is another SynapseGroup pointer,back_sg is another pygenn.SynapseGroup} with tranposed dimensions to sg i.e. its <i>postsynaptic</i> population has the same number of neurons as sg's <i>presynaptic</i> population and vice-versa.

After the update has run, any updates made to the 'forward' variable will also be applied to the tranpose variable.
\note 
Tranposing is currently only possible on variables belonging to synapse groups with SynapseMatrixType::DENSE_INDIVIDUALG connectivity.


-----
\link sectVariableInitialisation Previous\endlink | \link UserManual Top\endlink | \link sectSparseConnectivityInitialisation Next\endlink
*/
//----------------------------------------------------------------------------
/*! 
\page sectSparseConnectivityInitialisation Sparse connectivity initialisation

Synaptic connectivity implemented using SynapseMatrixConnectivity::SPARSE and SynapseMatrixConnectivity::BITMASK can be automatically initialised. 

This can be done using one of a number of predefined _sparse connectivity initialisation snippets_:
- InitSparseConnectivitySnippet::OneToOne
- InitSparseConnectivitySnippet::FixedProbability
- InitSparseConnectivitySnippet::FixedProbabilityNoAutapse
- InitSparseConnectivitySnippet::FixedNumberPostWithReplacement
- InitSparseConnectivitySnippet::FixedNumberTotalWithReplacement
- InitSparseConnectivitySnippet::FixedNumberPreWithReplacement
- InitSparseConnectivitySnippet::Conv2D

\add_toggle_python
In Python, these models can be selected by their unqualified name e.g. "FixedProbability".
\end_toggle
For example, to initialise synaptic connectivity with a 10% connection probability (allowing connections between neurons with the same id):
\add_toggle_code_cpp
InitSparseConnectivitySnippet::FixedProbability::ParamValues fixedProb(0.1);
    
model.addSynapsePopulation<...>(
        ...
        initConnectivity<InitSparseConnectivitySnippet::FixedProbability>(fixedProb));
\end_toggle_code
\add_toggle_code_python
fixed_prob = {"prob": 0.1}
model.add_synapse_population(
    ...
    genn_model.init_connectivity("FixedProbability", fixed_prob))
\end_toggle_code

\section sect_new_sparse_connect Defining a new sparse connectivity snippet
Similarly to variable initialisation snippets, sparse connectivity initialisation snippets can be created by simply defining a class in the model description.

For example, the following sparse connectivity initialisation snippet could be used to initialise a 'ring' of connectivity where each neuron is connected to a number of subsequent neurons specified using the ``numNeighbours`` parameter:
\add_toggle_code_cpp
class Ring : public InitSparseConnectivitySnippet::Base
{
public:
    DECLARE_SNIPPET(Ring, 1);

    SET_ROW_BUILD_STATE_VARS({{"offset", "unsigned int", 1}}});
    SET_ROW_BUILD_CODE(
        "const unsigned int target = ($(id_pre) + offset) % $(num_post);\n"
        "$(addSynapse, target);\n"
        "offset++;\n"
        "if(offset > (unsigned int)$(numNeighbours)) {\n"
        "   $(endRow);\n"
        "}\n");

    SET_PARAM_NAMES({"numNeighbours"});
    SET_CALC_MAX_ROW_LENGTH_FUNC(
        [](unsigned int numPre, unsigned int numPost, const std::vector<double> &pars)
        {
            return (unsigned int)pars[0];
        });
    SET_CALC_MAX_COL_LENGTH_FUNC(
        [](unsigned int numPre, unsigned int numPost, const std::vector<double> &pars)
        {
            return (unsigned int)pars[0];
        });
};
IMPLEMENT_SNIPPET(Ring);
\end_toggle_code
\add_toggle_code_python
ring_model = genn_model.create_custom_sparse_connect_init_snippet_class(
    "ring",
    param_names=["numNeighbours"],
    row_build_state_vars=[("offset", "unsigned int", 1)],
    row_build_code=
        """
        const unsigned int target = ($(id_pre) + offset) % $(num_post);
        $(addSynapse, target);
        offset++;
        if(offset > (unsigned int)$(numNeighbours)) {
           $(endRow);
        }
        """,

    calc_max_row_len_func=genn_model.create_cmlf_class(
        lambda num_pre, num_post, pars: int(pars[0]))(),
    calc_max_col_len_func=genn_model.create_cmlf_class(
        lambda num_pre, num_post, pars: int(pars[0]))())
\end_toggle_code
Each <i>row</i> of sparse connectivity is initialised independantly by running the snippet of code specified using the \add_cpp_python_text{`SET_ROW_BUILD_CODE()` macro,`row_build_code` keyword argument} within a loop.
The \$(num_post) variable can be used to access the number of neurons in the postsynaptic population and the \$(id_pre) variable can be used to access the index of the presynaptic neuron associated with the row being generated.
The \add_cpp_python_text{`SET_ROW_BUILD_STATE_VARS()` macro,`row_build_state_vars` keyword argument} can be used to initialise state variables outside of the loop - in this case ``offset`` which is used to count the number of synapses created in each row.
As well as allowing initialization of variables to a constant value, a code string can be used to initialize these values based on parameters, the index of the current row etc:
\add_toggle_code_cpp
SET_ROW_BUILD_STATE_VARS({{"inRow", "int", "($(id_pre) / (int)$(conv_ic)) / (int)$(conv_iw)"}});
\end_toggle_code
\add_toggle_code_python
row_build_state_vars=[("inRow", "int", "($(id_pre) / (int)$(conv_ic)) / (int)$(conv_iw)")],
\end_toggle_code
However, the rows generated by some connection initialisation algorithms such as InitSparseConnectivitySnippet::FixedNumberPreWithReplacement are not independent so cannot be generated in this way.
In these cases, code to build connectivity <i>columns</i> can be specified with the \add_cpp_python_text{`SET_COL_BUILD_CODE()` macro,`col_build_code` keyword argument} and the \add_cpp_python_text{`SET_COL_BUILD_STATE_VARS()` macro,`col_build_state_vars` keyword argument} can be used to initialise per-column state variables.
Synapses are added to the row using the \$(addSynapse, target) function and iteration is stopped using the \$(endRow) function.
\add_cpp_python_text{To avoid having to manually call SynapseGroup::setMaxConnections and SynapseGroup::setMaxSourceConnections\, sparse,Sparse} connectivity snippets should also provide code to calculate the maximum row and column lengths this connectivity will result in using the \add_cpp_python_text{SET_CALC_MAX_ROW_LENGTH_FUNC() and SET_CALC_MAX_COL_LENGTH_FUNC() macros, `calc_max_row_len_func` and `calc_max_col_len_func` keyword arguments`}.
\add_cpp_text{Alternatively\, if the maximum row or column length is constant\, the `SET_MAX_ROW_LENGTH()` and `SET_MAX_COL_LENGTH()` shorthand macros can be used.}

\section sect_sparse_kernel Kernel-based connectivity
Some forms of structured connectivity such as convolutions repeat a 'kernel' of weights across the entire connectivity matrix. 
Sparse connectivity initialisation snippets which operate in this way need to provide two extra pieces of information.
Firstly the dimensions of the kernel required by the connectivity are specified using the \add_cpp_python_text{SET_CALC_KERNEL_SIZE_FUNC() macro, `calc_kernel_size_func` keyword arguments`}:
\add_toggle_code_cpp
SET_CALC_KERNEL_SIZE_FUNC(
    [](const std::vector<double> &pars)->std::vector<unsigned int>
    {
        return {(unsigned int)pars[0], (unsigned int)pars[1],
                (unsigned int)pars[8], (unsigned int)pars[11]};
    });
\end_toggle_code
\add_toggle_code_python
calc_kernel_size_func=create_cksf_class(
        lambda pars: UnsignedIntVector([int(pars[0]), int(pars[1]), int(pars[8]), int(pars[11])]))(),
\end_toggle_code
Secondly, the row build code must use an extended version of the `addSynapse` function which indicates the kernel indices associated with the synapse.
For example, the following could be used with a 4 dimensional kernel: \$(addSynapse, idPost, kernRow, kernCol, inChan, outChan);
Finally, in order to use a kernel to initialise SynapseMatrixWeight::INDIVIDUAL variables or generate SynapseMatrixWeight::PROCEDURAL variables on the fly, the variables should be initialised using the InitVarSnippet::Kernel variable initialisation snippet and the kernel itself allocated as an extra global parameter and pushed to device (see \ref extraGlobalParamSim).

\section sect_sparse_connect_init_modes Sparse connectivity locations
Once you have defined <b>how</b> sparse connectivity is going to be initialised, similarly to variables, you can control <b>where</b> it is allocated. 
This is controlled using the same ``VarLocations`` options described in section \ref sect_var_init_modes and can either be set using the model default specifiued with ``ModelSpec::setDefaultSparseConnectivityLocation`` or on a per-synapse group basis using ``SynapseGroup::setSparseConnectivityLocation``.

-----
\link sectVariableReferences Previous\endlink | \link UserManual Top\endlink | \link Tutorial1 Next\endlink
*/
