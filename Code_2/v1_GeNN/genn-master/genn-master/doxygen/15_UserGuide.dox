
//----------------------------------------------------------------------------
/*! \page UserGuide Best practices guide

GeNN generates code according to the network model defined by the user as described in \ref UserManual\add_cpp_text{, and allows users to include the generated code in their programs as they want}. Here we provide a guideline to setup GeNN and use generated functions. 
We recommend users to also \add_cpp_python_text{have a look at the \ref Examples\, and to follow the tutorials \ref Tutorial1 and \ref Tutorial2,follow the tutorials \ref Tutorial1_Python and \ref Tutorial2_Python}.

\section Simulate Simulating a network model 
\add_toggle_cpp
Any variables marked as uninitialised using the ``uninitialisedVar()`` function or sparse connectivity not initialised using a snippet must be initialised by the user between calls to ``initialize()`` and ``initializeSparse()``.

Core functions generated by GeNN to be included in the user code include:

- `allocateMem()`
- `initialize()`
- `initializeSparse()`
- `stepTime()`
- `freeMem()`
- `getFreeDeviceMemBytes()`

In order to correctly access neuron state and spikes for the current timestep, correctly accounting for delay buffering etc, you can use the ``getCurrent<var name><neuron name>()``, ``get<neuron name>CurrentSpikes()`` and ``get<neuron name>CurrentSpikeCount()`` functions. Additionally, custom update groups (see \ref defining_custom_updates) can be simulated by calling ``update<group name>()``.
\end_toggle
\add_toggle_python
The pygenn.GeNNModel.build method can then be used to generate code for your model. 
Subsequently, the model can be loaded using pygenn.GeNNModel.load and simulated with pygenn.GeNNModel.step_time. Additionally, custom update groups (see \ref defining_custom_updates) can be simulated with pygenn.GeNNModel.custom_update. After calling pygenn.GeNNModel.load, the pygenn.GeNNModel.free_device_mem_bytes property can be used on supported hardware-accelerated backends to determine how much free device memory remains.
\end_toggle

By setting \add_cpp_python_text{``GENN_PREFERENCES::automaticCopy``, the `automaticCopy` keyword to pygenn.GeNNModel.__init__}, GeNN can be used in a simple mode where CUDA automatically transfers data between the GPU and CPU when required (see https://devblogs.nvidia.com/unified-memory-cuda-beginners/).
However, copying elements between the GPU and the host memory is costly in terms of performance and the automatic copying operates on a fairly coarse grain (pages are approximately 4 bytes).
Therefore, in order to maximise performance, we recommend you do not use automatic copying and instead manually call the following \add_cpp_python_text{functions,methods} when required:
\add_toggle_cpp
- `push<neuron or synapse name>StateToDevice()`
- `pull<neuron or synapse name>StateFromDevice()`
- `push<neuron name>SpikesToDevice()`
- `pull<neuron name>SpikesFromDevice()`
- `push<neuron name>SpikesEventsToDevice()`
- `pull<neuron name>SpikesEventsFromDevice()`
- `push<neuron name>SpikeTimesToDevice()`
- `pull<neuron name>SpikeTimesFromDevice()`
- `push<neuron name>CurrentSpikesToDevice()`
- `pull<neuron name>CurrentSpikesFromDevice()`
- `push<neuron name>CurrentSpikeEventsToDevice()`
- `pull<neuron name>CurrentSpikeEventsFromDevice()`
- `pull<synapse name>ConnectivityFromDevice()`
- `push<synapse name>ConnectivityToDevice()`
- `pull<var name><neuron or synapse name>FromDevice()`
- `push<var name><neuron or synapse name>ToDevice()`
- `pushCurrent<var name><neuron name>ToDevice()`
- `pullCurrent<var name><neuron name>FromDevice()`
- `getCurrent<var name><neuron name>()`
- `copyStateToDevice()` 
- `copyStateFromDevice()`
- `copyCurrentSpikesFromDevice()`
- `copyCurrentSpikesEventsFromDevice()`

\end_toggle
\add_toggle_python
- pygenn.genn_groups.Group.pull_state_from_device
- pygenn.genn_groups.Group.pull_var_from_device
- pygenn.genn_groups.Group.push_state_to_device
- pygenn.genn_groups.Group.push_var_to_device
- pygenn.NeuronGroup.pull_spikes_from_device
- pygenn.NeuronGroup.pull_spike_events_from_device
- pygenn.NeuronGroup.pull_current_spikes_from_device
- pygenn.NeuronGroup.pull_current_spike_events_from_device
- pygenn.NeuronGroup.push_spikes_to_device
- pygenn.NeuronGroup.push_spike_events_to_device
- pygenn.NeuronGroup.push_current_spikes_to_device
- pygenn.NeuronGroup.push_current_spike_events_to_device
- pygenn.SynapseGroup.pull_connectivity_from_device
- pygenn.SynapseGroup.push_connectivity_to_device

\end_toggle
You can use \add_cpp_python_text{``push<neuron or synapse name>StateToDevice()``,pygenn.genn_groups.Group.push_state_to_device} to copy from the host to the GPU.
At the end of your simulation, if you want to access the variables you need to copy them back from the device using the \add_cpp_python_text{``pull<neuron or synapse name>StateFromDevice()`` function, pygenn.genn_groups.Group.pull_state_from_device method} or one of the more fine-grained functions listed above. 

\subsection extraGlobalParamSim Extra Global Parameters
If extra global parameters have a "scalar" type such as ``float`` they can be set directly from simulation code. For example the extra global parameter "reward" of \add_cpp_python_text{population "Pop" can be set with,pygenn.NeuronGroup "pop" should first be initialised before pygenn.GeNNModel.load is called with}:
\add_toggle_code_cpp
rewardPop = 5.0f;
\end_toggle_code
\add_toggle_code_python
pop.set_extra_global_param("reward", 5.0)
\end_toggle_code
\add_toggle_python
and subsequently can be updated with:
\code
pop.extra_global_params["reward"].view[:] = 5.0
\endcode
\end_toggle
\add_toggle_cpp
However, if extra global parameters have a pointer type such as ``float*``, GeNN generates additional functions to allocate, free and copy these extra global parameters between host and device:
- `allocate<egp name><neuron or synapse name>`
- `free<egp name><neuron or synapse name>`
- `push<egp name><neuron or synapse name>ToDevice`
- `pull<egp name><neuron or synapse name>FromDevice`
These operate in much the same manner as the functions for interacting with standard variables described above but the allocate, push and pull functions all take a "count" parameter specifying how many entries the extra global parameter array should be.
\end_toggle
\add_toggle_python
Extra global parameters with a pointer type such as ``float*`` should be initialised and updated in the same manner but, if their value is changed after pygenn.GeNNModel.load is called, the updated values need to be pushed to the GPU:
\code
pop.extra_global_params["reward"].view[:] = [1,2,3,4]
pop.push_extra_global_param_to_device("reward", 4)
\endcode
\end_toggle
Extra global parameters can also be used to provide additional data to snippets used for variable (see \ref sectVariableInitialisation) or sparse connectivity (see \ref sectSparseConnectivityInitialisation) initialisation.
\add_toggle_cpp
Like standard extra global parameters, GeNN generates additional functions to allocte, free and copy these extra global parameters between host and device:
    - `allocate<egp name><var name><neuron or synapse name>`
    - `free<egp name><var name><neuron or synapse name>`
    - `push<egp name><var name><neuron or synapse name>ToDevice`
    - `pull<egp name><var name><neuron or synapse name>FromDevice`
\end_toggle
\add_toggle_python
These extra global parameters must be initialised before pygenn.GeNNModel.load is called:
\code
pop.vars["g"].set_extra_global_init_param("kernel", [1, 2, 3, 4])
\endcode
\end_toggle
\section floatPrecision Floating point precision

Double precision floating point numbers are supported by devices with compute capability 1.3 or higher. If you have an older GPU, you need to use single precision floating point in your models and simulation. 
Furthermore, GPUs are designed to work better with single precision while double precision is the standard for CPUs. This difference should be kept in mind while comparing performance.

Typically, variables in GeNN models are defined using the `scalar` type. This type is substituted with "float" or "double" during code generation, according to the model precision. This is specified \add_cpp_python_text{with ModelSpec::setPrecision() -- either `GENN_FLOAT` or `GENN_DOUBLE`. `GENN_FLOAT` is the default value,with the first parameter to pygenn.GeNNModel.__init__ as a string e.g. "float"}.
 
There may be ambiguities in arithmetic operations using explicit numbers. Standard C compilers presume that any number defined as "X" is an integer and any number defined as "X.Y" is a double. Make sure to use the same precision in your operations in order to avoid performance loss.

\section ListOfVariables Working with variables in GeNN

\subsection modelVars Model variables
User-defined model variables originate from classes derived off the NeuronModels::Base, WeightUpdateModels::Base or PostsynapticModels::Base classes. The name of model variable is defined in the model type, i.e. with a statement such as
\add_toggle_code_cpp
SET_VARS({{"V", "scalar"}});
\end_toggle_code
\add_toggle_code_python
var_name_types=[("V", "scalar")]
\end_toggle_code
\add_toggle_cpp
When a neuron or synapse population using this model is added to the model, the full GeNN name of the variable will be obtained by concatenating the variable name with the name of the population.
For example if we add a population called `Pop` using a model which contains our `V` variable, a variable `VPop` of type `scalar*` will be available in the global namespace of the simulation program. GeNN will pre-allocate this C array to the correct size of elements corresponding to the size of the neuron population. Users can otherwise manipulate these variable arrays as they wish.
\end_toggle
\add_toggle_python
When a neuron or synapse population using this model is added to the model, it is built (with pygenn.GeNNModel.build) and loaded (with pygenn.GeNNModel.load), it is available to Python code via a numpy memory view into the host memory:
\code
pop.vars["V"].view[:] = 1.2
\endcode
\end_toggle
For convenience, GeNN provides functions to copy each state variable from the device into host memory and vice versa e.g. \add_cpp_python_text{`pullVPopFromDevice()` and `pushVPoptoDevice()`,pygenn.genn_groups.Group.pull_var_from_device and pygenn.genn_groups.Group.push_var_to_device}.
Alternatively, all state variables associated with a population can be copied using a single call E.g.
\add_toggle_code_cpp
pullPopStateFromDevice();
\end_toggle_code
\add_toggle_code_python
pop.pull_state_from_device()
\end_toggle_code
These conventions also apply to the the variables of postsynaptic  and weight update models.
\add_toggle_cpp
\note 
Be aware that the above naming conventions do assume that variables from the weightupdate models and the postSynModels that are used together in a synapse population are unique. If both the weightupdate model and the postSynModel have a variable of the same name, the behaviour is undefined.
\end_toggle

\subsection predefinedVars Built-in Variables in GeNN

GeNN has no explicitly hard-coded synapse and neuron variables. Users are free to name the variable of their models as they want. However, there are some reserved variables that are used for intermediary calculations and communication between different parts of the generated code. They can be used in the user defined code but no other variables should be defined with these names.

- \c DT : Time step (typically in ms) for simulation;
Neuron integration can be done in multiple sub-steps inside the neuron model for numerical stability (see Traub-Miles and Izhikevich neuron model variations in \ref sectNeuronModels).

- \c inSyn: This is an intermediary synapse variable which contains the summed input into a postsynaptic neuron (originating from the \$(addToInSyn, X) or \$(addToInSynDelay, X, Y) functions of the weight update model used by incoming synapses) . 

- \c Isyn : This is a local variable which contains the (summed) input current to a neuron. It is typically the sum of any explicit current input 
and all synaptic inputs. The way its value is calculated during the update of the postsynaptic neuron is defined by the code provided in the postsynaptic model. For example, the standard PostsynapticModels::ExpCond postsynaptic model defines
\add_toggle_code_cpp
SET_APPLY_INPUT_CODE("$(Isyn) += $(inSyn)*($(E)-$(V));");
\end_toggle_code
\add_toggle_code_python
apply_input_code="$(Isyn) += $(inSyn)*($(E)-$(V));"
\end_toggle_code
which implements a conductance based synapse in which the postsynaptic current is given by \f$I_{\rm syn}= g*s*(V_{\rm rev}-V_{\rm post})\f$.
The value of \$(Isyn) resulting from the apply input code can then be used in neuron sim code like so:
\code
$(V)+= (-$(V)+$(Isyn))*DT 
\endcode 

- \c sT : This is a neuron variable containing the spike time of each neuron and is automatically generated for pre and postsynaptic neuron groups if they are connected using a synapse population with a weight update model that has \add_cpp_python_text{SET_NEEDS_PRE_SPIKE_TIME(true) or SET_NEEDS_POST_SPIKE_TIME(true),`is_pre_spike_time_required=True` or `is_post_spike_time_required=True`} set.
- \c prev_sT: This is a neuron variable containing the _previous_ spike time of each neuron and is automatically generated for pre and postsynaptic neuron groups if they are connected using a synapse population with a weight update model that has \add_cpp_python_text{SET_NEEDS_PREV_PRE_SPIKE_TIME(true) or SET_NEEDS_PREV_POST_SPIKE_TIME(true),`is_prev_pre_spike_time_required=True` or `is_prev_post_spike_time_required=True`} set.

In addition to these variables, neuron variables can be referred to in the synapse models by calling $(\<neuronVarName\>_pre) for the presynaptic neuron population, and $(\<neuronVarName\>_post) for the postsynaptic population. For example, \$(sT_pre), \$(sT_post), \$(V_pre), etc.

\section spikeRecording Spike Recording
Especially in models simulated with small timesteps, very few spikes may be emitted every timestep, making calling \add_cpp_python_text{``pull<neuron name>CurrentSpikesFromDevice()`` or ``pull<neuron name>SpikesFromDevice()``, pygenn.NeuronGroup.pull_current_spikes_from_device} every timestep very inefficient.
Instead, the spike recording system allows spikes and spike-like events emitted over a number of timesteps to be collected in GPU memory before transferring to the host.
Spike recording can be enabled on chosen neuron groups with the \add_cpp_python_text{``NeuronGroup::setSpikeRecordingEnabled`` and ``NeuronGroup::setSpikeEventRecordingEnabled`` methods,pygenn.NeuronGroup.spike_recording_enabled and pygenn.NeuronGroup.spike_event_recording_enabled properties}.
Remaining GPU memory can then be allocated at runtime for spike recording by\add_cpp_python_text{calling ``allocateRecordingBuffers(<number of timesteps>)`` from user code,using the `num_recording_timesteps` keyword argument to pygenn.GeNNModel.load}.
The data structures can then be copied from the GPU to the host using the \add_cpp_python_text{``pullRecordingBuffersFromDevice()`` function,pygenn.GeNNModel.pull_recording_buffers_from_device method} and the spikes emitted by a population can be accessed \add_cpp_python_text{in bitmask form via the ``recordSpk<neuron name>`` variable,via the pygenn.NeuronGroup.spike_recording_data property}
Similarly, spike-like events emitted by a population can be accessed via the \add_cpp_python_text{``recordSpkEvent<neuron name>`` variable,pygenn.NeuronGroup.spike_event_recording_data property}. 
\add_cpp_text{To make decoding the bitmask data structure easier, the ``::writeBinarySpikeRecording`` and ``::writeTextSpikeRecording`` helper functions can be used by including spikeRecorder.h in the user code.}

\section Debugging Debugging suggestions
\add_toggle_cpp
In Linux, users can call `cuda-gdb` to debug on the GPU. Example projects in the `userproject` directory come with a flag to enable debugging (--debug). genn-buildmodel.sh has a debug flag (-d) to generate debugging data.
If you are executing a project with debugging on, the code will be compiled with -g -G flags. In CPU mode the executable will be run in gdb,
and in GPU mode it will be run in cuda-gdb in tui mode. 

\note
Do not forget to switch debugging flags -g and -G off after debugging is complete as they may negatively affect performance.

On Mac, some versions of `clang` aren't supported by the CUDA toolkit. This is a recurring problem on Fedora as well, where CUDA doesn't keep up with GCC releases. You can either hack the CUDA header which checks compiler versions - `cuda/include/host_config.h` - or just use an older XCode version (6.4 works fine).

On Windows models can also be debugged and developed by opening the sln file used to build the model in Visual Studio. From here files can be added to the project, build settings can be adjusted and the full suite of Visual Studio debugging and profiling tools can be used. 
\note
When opening the models in the `userproject` directory in Visual Studio, right-click on the project in the solution explorer, select 'Properties'. Then, making sure the desired configuration is selected, navigate to 'Debugging' under 'Configuration Properties', set the 'Working Directory' to '..' and the 'Command Arguments' to match those passed to genn-buildmodel e.g. 'outdir' to use an output directory called outdir.
\end_toggle
\add_toggle_python
To build a debug version of PyGeNN, you first need to build debug dynamic libraries.
On Linux, these can be built directly into the PyGeNN directory:
\code
 make DEBUG=1 DYNAMIC=1 LIBRARY_DIRECTORY=`pwd`/pygenn/genn_wrapper/
\endcode
On Windows, building these requires two steps:
\code
 msbuild genn.sln /t:Build /p:Configuration=Debug_DLL
 copy /Y lib\genn*Debug_DLL.* pygenn\genn_wrapper
\endcode
Finally the debug Python extension can be built with setup tools using:
\code
 python setup.py build_ext --debug develop
\endcode
\end_toggle

-----
\link Tutorial2_Python Previous\endlink | \link UserGuide Top\endlink | \link Credits Next\endlink
*/
